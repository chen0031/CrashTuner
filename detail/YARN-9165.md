# YARN-9165

## Scenario

<div  align="center">    
 <img src="https://github.com/lujiefsi/CrashTuner/blob/master/pictures/9165.png" width="100%" height="100%" alt="YARN-9165" align=center />
</div>


1. YARN start to allocate a container on a node.
2. YARN will get a node by NodeId.
3. If one node just crashes, recovery process will clean all nodes.
4. So no node is obtained, but yarn still use it, hence NPE happens.

## Trigger analysis

```
 public N getNode(NodeId nodeId) {
   readLock.lock();
   try {
     return nodes.get(nodeId);
   } finally {
     readLock.unlock();
   }
 }
```

1. Above function code is the place where we read the nodes.
2. This function has about 43 different contexts.
3. About 25 contexts have sanity check, so **context sensitive** is so import to trigger this bug.
4. Above code also has lock, moving the crash point to caller can also help us **avoid lock**. 

## Crash Point

org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils createOpportunisticRmContainer 566
